---
status: publish
published: true
title: A Comparison of Knowledge Extraction Tools for the Semantic Web
wordpress_id: 674
wordpress_url: http://blog.semanticlab.net/?p=674
date: '2013-10-02 15:47:46 +0200'
date_gmt: '2013-10-02 14:47:46 +0200'
categories:
- Information Extraction
tags:
- evaluation
- named entity recognition
- named entity linking
comments: []
---
<p>Gangemi, A., 2013. A Comparison of Knowledge Extraction Tools for the Semantic Web. In P. Cimiano et al., eds. The Semantic Web: Semantics and Big Data. Lecture Notes in Computer Science. Springer Berlin Heidelberg, pp. 351&mdash;366.</p>
<h2>Summary</h2>
This article compares a selection of natural language processing (NLP) tools for the following basic natural language processing tasks (in contrast to advanced tasks such as question answering, retrieval, etc.):</p>
<ol>
<li>topic extraction</li>
<li>named entity recognition - i.e. determine entity types such as person, organization, etc.</li>
<li>named entity resolution (or linking) - i.e. provide a reference to the individual mentioned</li>
<li>named entity coreference</li>
<li>terminology extraction (typically for classes and or properties)</li>
<li>sense tagging</li>
<li>sense disambiguation</li>
<li>taxonomy induction</li>
<li>relation extraction</li>
<li>semantic role labeling (property induction for events and n-ary relations)</li>
<li>event detection</li>
<li>frame detection</li>
</ol></p>
<h2>Evaluation</h2>
The authors provide a comparison of the tools' performance (precision, recall, accuracy, f1) on the NLP tasks mentioned above. These measures are determined based on a single gold standard evaluation document. The gold standard entries have been constructed by merging and cleaning the results of multiple tools - an approach that has been inspired by information retrieval with incomplete information [1].</p>
<h2>Further Literature</h2></p>
<div>
<div>
<div>
<div>1. Buckley, C. &amp; Voorhees, E.M., 2004. Retrieval evaluation with incomplete information. In <em>Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</em>. SIGIR  &trade;04. New York, NY, USA: ACM, pp. 25&mdash;32.</div>
</div>
</div>
</div>
&nbsp;</p>
